# Module 4: Supervised Learning Fundamentals üìà

[‚Üê Previous Module](03_intro_to_ml.md) | [Back to Main](../README.md) | [Next Module ‚Üí](05_unsupervised_learning.md)

## üìã Table of Contents
1. [Linear Regression](#linear-regression)
2. [Logistic Regression](#logistic-regression)
3. [Support Vector Machines](#support-vector-machines)
4. [Gradient Descent](#gradient-descent)
5. [Regularization Techniques](#regularization-techniques)
6. [Model Evaluation](#model-evaluation)
7. [Cross-Validation Strategies](#cross-validation-strategies)
8. [Practical Applications](#practical-applications)

---

## Overview

This module covers the fundamental supervised learning algorithms that form the backbone of machine learning. You'll learn both regression and classification techniques, optimization methods, and how to properly evaluate and validate your models.

## Key Topics Covered

### Linear Regression
- Simple and Multiple Linear Regression
- Ordinary Least Squares (OLS)
- Assumptions and diagnostics
- Polynomial regression
- Implementation from scratch and with scikit-learn

### Logistic Regression
- Binary and multiclass classification
- Sigmoid function and log-odds
- Maximum likelihood estimation
- Decision boundaries
- Softmax regression

### Support Vector Machines (SVM)
- Maximum margin classifiers
- Kernel trick
- Soft margin and C parameter
- Multi-class strategies
- SVM for regression (SVR)

### Gradient Descent
- Batch gradient descent
- Stochastic gradient descent (SGD)
- Mini-batch gradient descent
- Learning rate scheduling
- Momentum and adaptive methods (Adam, RMSprop)

### Regularization
- L1 Regularization (Lasso)
- L2 Regularization (Ridge)
- Elastic Net
- Dropout (preview for neural networks)
- Early stopping

### Best Practices
- Feature scaling and normalization
- Handling imbalanced datasets
- Feature selection methods
- Ensemble techniques preview

## Prerequisites
- Completed Modules 1-3
- Understanding of linear algebra and calculus basics
- Familiarity with Python and NumPy

## Learning Outcomes
By the end of this module, you will be able to:
- Implement supervised learning algorithms from scratch
- Choose appropriate algorithms for different problems
- Apply regularization to prevent overfitting
- Optimize models using gradient descent
- Properly evaluate model performance

---

[**Continue to Module 5: Unsupervised Learning ‚Üí**](05_unsupervised_learning.md)